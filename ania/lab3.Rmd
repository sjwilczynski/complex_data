---
title: "Complex Data lab 3"
author: "Anna Zaleska"
header-includes:
   - \usepackage{xcolor}
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(nlme)
library(lattice)
```

### Data input and calculating means
```{r}
wtloss <- read.table("../data/weightloss.dat",
header=F)

## Give names to variables
names(wtloss) <- c("id", paste("y", 1:4, sep=""), "program")

## Univariate format
wtloss.uni <- data.frame(id=rep(wtloss$id, each=4),
wgtloss=as.numeric(t(as.matrix(wtloss[,2:5]))),
program=rep(wtloss$program, each=4),
month=seq(0,9,3),
time.cat=rep(1:4))

wtloss.uni$prog.fac <- factor(wtloss.uni$program, labels=c("1:encourage", "2:none"))
attach(wtloss.uni)
wgt.mean <- tapply(wgtloss, list(month, program), mean)
wgt.sd <- tapply(wgtloss, list(month, program), sd)
wgt.mean
wgt.sd
detach(wtloss.uni)
```


### Plotting response profiles
```{r}
xyplot(wgtloss~month|prog.fac, type='l',groups=id,data=wtloss.uni)
```

#### The most general mean models with different covariance classes:

```{r}

## CATEGORICAL time, UN, REML
wtloss.un.cat <- gls(wgtloss~factor(month)*prog.fac,
correlation=corSymm(form= ~1 | id),
weights=varIdent(form= ~1 | month),
data=wtloss.uni)
#summary(wtloss.un.cat)

## CATEGORICAL time, CS, REML
wtloss.cs.cat <- gls(wgtloss~factor(month)*prog.fac,
correlation=corCompSymm(form= ~1 | id),
weights=varIdent(form= ~1),
data=wtloss.uni)
#summary(wtloss.cs.cat)

## CATEGORICAL time, AR1, REML
wtloss.ar1.cat <- gls(wgtloss~factor(month)*prog.fac,
correlation=corAR1(form= ~1 | id),
weights=varIdent(form= ~1),
data=wtloss.uni)
#summary(wtloss.ar1.cat)
```

### Unstructured vs compound symmetry
$H_0:$ compound symmetry is adequate for the data \newline
$H_1:$ unstructred is required
```{r}
anova(wtloss.un.cat, wtloss.cs.cat)
```




\begin{center}
  \begin{tabular}{ l  c c }
  \multicolumn{3}{c}{CS vs. UN} \\
    \hline \hline
     & -2 REML & Number of \\ 
    Structure & Log-Likelihood & Cov. Parameters \\ \hline
    Compound Symmetry & `r -2*(-952.9116)` & 10 \\
    Unstructured & `r -2*(-950.4383)` &  18 \\
    Difference & `r -2*(-952.9116) -(-2*(-950.4383)) ` & 8 \\
    \hline
  \end{tabular}
\end{center}

LRT yields $G^2$ =  `r -2*(-952.9116) -(-2*(-950.4383)) ` with 8 df (p =0.7633), so we do not reject the null hypothesis
at $\alpha = 0.05$ and conclude that the assumption of compound symmetry
covariance structure is adequate for the data.



### Unstructured vs autoregressive
$H_0:$ autoregressive is adequate for the data \newline
$H_1:$ unstructred is required

```{r}
anova(wtloss.un.cat, wtloss.ar1.cat)
```

\begin{center}
  \begin{tabular}{ l  c c }
  \multicolumn{3}{c}{AR-1 vs. UN} \\
    \hline \hline
     & -2 REML & Number of \\ 
    Structure & Log-Likelihood & Cov. Parameters \\ \hline
    Autoregressive & `r -2*(-965.6234)` & 10 \\
    Unstructured & `r -2*(-950.4383)` &  18 \\
    Difference & `r -2*(-965.6234) -(-2*(-950.4383)) ` & 8 \\
    \hline
  \end{tabular}
\end{center}

LRT yields $G^2$ =  `r -2*(-965.6234) -(-2*(-950.4383)) ` with 8 df (p =0.0002), so we  reject the null hypothesis at $\alpha = 0.05$ and conclude  that the assumption of autoregressive covariance structure is inappropriate  when compared to unstructured


###Autoregressive vs compound symmetry

Since CS and AR-1 have the same number of parameters = 10 , no LRT is necessary.  We can directly compare their likelihoods, or -2*log(likelihood):
\begin{itemize}
\item $-2*log(likelihood)$ for CS = `r -2*(-952.9116)`
\item $-2*log(likelihood)$ for AR-1 = `r -2*(-965.6234) `
\end{itemize}
Since $-2*log(likelihood)$ for CS is smaller  than for AR-1, CS has a higher likelihood
and we conclude that CS is an  adequate model for the covariance
structure when compared to AR-1.

\textbf{The most adequate covariance structure is compound symmetry model.}

Are all these tests correct? \newline
anova(wtloss.un.cat, wtloss.ar1.cat) \newline
anova(wtloss.un.cat, wtloss.cs.cat) \newline
anova(wtloss.ar1.cat, wtloss.cs.cat) \newline

First two tests are correct. The last one is theoretically incorrect because autoregressive and compound symmetry models are not nested and we should not compare their log-likelihoods or perform LRT on them. In such a case we can compare AIC to find the best model but in our situation both models have the same number of parameters so comparing AICs is the same as comparing likelihoods and thus we can do it.


\begin{center}
  \begin{tabular}{ l  c c c }
    \hline \hline
     & -2 REML & Number of  &\\ 
    Structure & Log-Likelihood & Cov. Parameters  & AIC\\ \hline
    Compoud Symmetry &`r -2*(-952.9116)`& 10 & 1925.823 \\
    Autoregressive & `r -2*(-965.6234)` & 10 & 1951.247\\
    Unstructured & `r -2*(-950.4383)` &  18 & 1936.877 \\

    \hline
  \end{tabular}
\end{center}

Thus, we will use a compound symmetry covariance structure for the remainder of the lab

\newpage

### AUC - test for equality of the area under the curve in two groups.

```{r}
t <- wtloss.uni$month
L2 <- 0.5*c(t[1] + t[2] - 2*t[4], t[3] - t[1], t[4]-t[2], t[4]-t[3])
coefs <- summary(wtloss.cs.cat)$coefficients
AUC_enc <- sum(L2 * wgt.mean[,1])
AUC_none <- sum(L2 * wgt.mean[,2])
```

Estimated mean AUC in encouragement program is `r AUC_enc`. \newline
Estimated mean AUC in no encouragement program is `r AUC_none`. \newline

Testing for the equality of AUCs:\newline
We will use the contrast $(-L_2,L_2)$.

```{r}
L <- c(-L2, L2)
anova(wtloss.cs.cat, L=L)
```

Because of p-value which is < 0.0001 we reject the null hypothesis and conclude that response profile is the same for both treatments.

\newpage

### Parametric curves 
#### Quadratic time trend

```{r}
## QUADRATIC time, CS, REML,
wtloss.cs.quad <- gls(wgtloss~month*prog.fac + I(month^2)*prog.fac,
correlation=corCompSymm(form= ~1 | id),
weights=varIdent(form= ~1),
data=wtloss.uni,
method="ML")
#summary(wtloss.cs.quad)

## CATEGORICAL time, CS, ML
wtloss.cs.cat.ml <- gls(wgtloss~factor(month)*prog.fac,
correlation=corCompSymm(form= ~1 | id),
weights=varIdent(form= ~1),
data=wtloss.uni,
method="ML")
#summary(wtloss.cs.cat.ml)
```

$H_0:$ quadratic model \newline
$H_1:$  saturated mode

```{r}
anova(wtloss.cs.cat.ml, wtloss.cs.quad)
```

\begin{center}
  \begin{tabular}{ l  c c }
  \multicolumn{3}{c}{Testing the Quadratic trend} \\
    \hline \hline
     & -2 Log & Number of \\ 
    Structure & Likelihood & Cov. Parameters \\ \hline
    Quadratic model & `r -2*(-967.85)` & 8 \\
    Saturated model & `r -2*(-967.6657)` &  10 \\
    Difference & `r -2*(-967.85) -(-2*(-967.6657)) ` & 2 \\
    \hline
  \end{tabular}
\end{center}

LRT yields G2 =0.369 (p = 0.8316), so we fail to reject the null hypothesis at
$\alpha = 0.05$ and conclude that the model with Month as a quadratic effect
seems to fit the data adequately. (Note: $chi_{2,0.95}^2 = 5.99$)

#### Linear time trend

```{r}
wtloss.cs.lin <- gls(wgtloss~month*prog.fac,
correlation=corCompSymm(form= ~1 | id),
weights=varIdent(form= ~1),
data=wtloss.uni,
method="ML")
#summary(wtloss.cs.lin)
```



$H_0:$ linear model \newline
$H_1:$ quadratic model



```{r}
anova(wtloss.cs.lin, wtloss.cs.quad)
```

\begin{center}
  \begin{tabular}{ l  c c }
  \multicolumn{3}{c}{Linear vs. Quadratic} \\
    \hline \hline
     & -2 Log & Number of \\ 
    Structure & Likelihood & Cov. Parameters \\ \hline
    Linear model & `r -2*(-968.1477)` & 6 \\
    Quadratic model & `r -2*(-967.85)` & 8 \\
    Difference & `r -2*(-968.1477) -(-2*(-967.85)) ` & 2 \\
    \hline
  \end{tabular}
\end{center}

LRT yields G2 =0.596 (p = 0.7425), so we fail to reject the null hypothesis at
$\alpha = 0.05$ and conclude that the model with Month as a linear effect
fits the data better than the model with Month as a quadratic effect. (Note: $chi_{2,0.95}^2 = 5.99$)






#### Testing for intersections 

```{r}
## Linear model, NO interacations
wtloss.cs.lin.noint <- gls(wgtloss~month + prog.fac,
correlation=corCompSymm(form= ~1 | id),
weights=varIdent(form= ~1),
data=wtloss.uni,
method="ML")
#summary(wtloss.cs.lin.noint)
```

$H_0: \beta_4 = 0$ \newline
$H_1:  \beta_4 \neq 0$

```{r}
anova(wtloss.cs.lin, L= c(0,0,0,1))
```

Since the p-value for month*program is $<0.0001$, we reject the null hypothesis and
conclude that there is an interaction between month and program. \newline

Thus, our final model is the linear model with interaction. \newline

```{r}
wtloss.cs.lin.final <- gls(wgtloss~month*prog.fac,
correlation=corCompSymm(form= ~1 | id),
weights=varIdent(form= ~1),
data=wtloss.uni)
summary(wtloss.cs.lin.final)$coefficients
```

What can you conclude about the two weight programs in terms of their
effectiveness? \newline
Let us look at estimated rate of change for both programs. \newline
First program:  $\hat\beta_2 =$ `r wtloss.cs.lin.final$coefficients[2]` \newline
Second program:  $\hat\beta_2 + \hat\beta_4 =$ `r wtloss.cs.lin.final$coefficients[2] + wtloss.cs.lin.final$coefficients[4]` \newline
We can conclude that program with receiving encouragement is much more effective than the one without.

\newpage

#### Interpreting quadratic trends:
```{r}
wtloss.cs.quad.noint <- gls(wgtloss~month + prog.fac + I(month^2),
correlation=corCompSymm(form= ~1 | id),
weights=varIdent(form= ~1),
data=wtloss.uni)

coefficients.m <- wtloss.cs.quad.noint$coefficients
coefficients <- c(coefficients.m[1], coefficients.m[2], coefficients.m[4], coefficients.m[3])
change_rates <- coefficients[2] + 2*coefficients[3]*t[1:4] 
expected1 <- coefficients[1] + coefficients[2]*t[1:4] + coefficients[3]*t[1:4]*t[1:4]
```
$\hat{\beta_1}$ = `r coefficients[1]` expected mean response at baseline of subjects in program 1. \newline
$\hat{\beta_4}$= `r coefficients[4]` change in expected response at baseline of subjects in program 2 vs.
program 1.\newline
Rate of change in program 1 is $\beta_2 + 2\beta_3 time_{ij}$ . \newline
Plugging in the above estimates,

\begin{center}
  \begin{tabular}{ l  c c }
  \multicolumn{3}{c}{Program 1} \\
    \hline \hline
     & Rate of & Expected \\ 
    time & Change & Response \\ \hline
    0 & `r change_rates[1]` & `r expected1[1]` \\
    1 & `r change_rates[2]` & `r expected1[2]` \\
    2 & `r change_rates[3]` & `r expected1[3]` \\
    3 & `r change_rates[4]` & `r expected1[4]` \\
    \hline
  \end{tabular}
\end{center}


Thus, the mean response for Program 1 decreases over time. For the second program effect is the same. We need to add $\hat{\beta_4}$ to the expected response at each time point but the trend remains.


